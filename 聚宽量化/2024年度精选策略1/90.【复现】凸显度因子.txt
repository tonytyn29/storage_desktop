#!/usr/bin/env python
# coding: utf-8

# In[1]:


import empyrical as ep
import pandas as pd
import numpy as np
import qlib
from qlib.data import D
from qlib.workflow import R  # å®éªŒè®°å½•ç®¡ç†å™¨
# from qlib.workflow.record_temp import PortAnaRecord, SigAnaRecord, SignalRecord
from qlib.data.dataset.loader import StaticDataLoader
from qlib.data.dataset.handler import DataHandlerLP
from qlib.data.dataset import DatasetH
from qlib.data.dataset.processor import DropnaLabel, ProcessInf, CSRankNorm, Fillna
# from qlib.utils import init_instance_by_config
from typing import List, Tuple, Dict

from scr.core import calc_sigma, calc_weight
from scr.factor_analyze import clean_factor_data, get_factor_group_returns
from scr.qlib_workflow import run_model
from scr.plotting import model_performance_graph, report_graph

import matplotlib.pyplot as plt
import seaborn as sns

# pltä¸­æ–‡æ˜¾ç¤º
plt.rcParams["font.sans-serif"] = ["SimHei"]
# pltæ˜¾ç¤ºè´Ÿå·
plt.rcParams["axes.unicode_minus"] = False


# In[2]:


qlib.init(provider_uri="qlib_data", region="cn")


# In[3]:


# ä½¿ç”¨D.featureä¸DataLoader,DataHandlerLP,DatasetHè·å–æ•°æ®çš„æ•°æ®MutiIndexç´¢å¼•ä¸åŒ
# å‰è€…Instrument,datetimeåè€…æ˜¯datetime,Instrument
POOLS: List = D.list_instruments(D.instruments("pool"), as_list=True)
pct_chg: pd.DataFrame = D.features(POOLS, fields=["$close/Ref($close,1)-1"])
pct_chg: pd.DataFrame = pct_chg.unstack(level=0)["$close/Ref($close,1)-1"]

# æœªæ¥æœŸæ”¶ç›Š
next_ret: pd.DataFrame = D.features(POOLS, fields=["Ref($open,-2)/Ref($open,-1)-1"])
next_ret.columns = ["next_ret"]
next_ret: pd.DataFrame = next_ret.swaplevel()
next_ret.sort_index(inplace=True)

# åŸºå‡†
bench: pd.DataFrame = D.features(["000300.SH"], fields=["$close/Ref($close,1)-1"])
bench: pd.Series = bench.droplevel(level=0).iloc[:, 0]


# # åŸå§‹æ„é€ 
# 
# ## ç†è®ºåŸºç¡€
# 
# æœ‰æ•ˆå¸‚åœºå‡è¯´è®¤ä¸ºè‚¡ç¥¨ä»·æ ¼åæ˜ äº†æ‰€æœ‰å¯ç”¨ä¿¡æ¯ï¼ŒæŠ•èµ„è€…æ— æ³•é€šè¿‡è§‚å¯Ÿå¸‚åœºå˜åŒ–æˆ–è€…åˆ†æå¸‚åœºæ•°æ®æ¥é¢„æµ‹æœªæ¥
# è‚¡ç¥¨ä»·æ ¼çš„èµ°åŠ¿ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä»æœ‰å¤§é‡çš„å®è¯ç ”ç©¶è¡¨æ˜é‡‘èå¸‚åœºä¸­å­˜åœ¨è®¸å¤šèµ„äº§å®šä»·æ¨¡å‹æ‰€æ— æ³•è§£é‡Šçš„å¼‚è±¡ã€‚ä¸ºäº†
# è§£é‡Šè¿™äº›å¼‚è±¡ï¼Œè®¸å¤šå­¦è€…å¼€å§‹ä»è¡Œä¸ºé‡‘èå­¦çš„è§’åº¦å¯¹æŠ•èµ„è€…è¿›è¡ŒæŠ•èµ„å†³ç­–æ—¶çš„å¿ƒç†å±•å¼€ç ”ç©¶ï¼Œæ•°åå¹´æ¥æ¶Œç°äº†å¤§é‡
# é«˜è´¨é‡çš„è¡Œä¸ºé‡‘èå­¦å®è¯ç ”ç©¶æ–‡çŒ®ã€‚
# 
# åœ¨è¡Œä¸ºé‡‘èå­¦é¢†åŸŸä¸­ï¼Œæœ€å…·ä»£è¡¨æ€§çš„äººç‰©ä¹‹ä¸€è«è¿‡äº 2002 å¹´å› æå‡ºå‰æ™¯ç†è®ºï¼ˆProspect Theory, 1979ï¼‰è€Œè·å¾—
# è¯ºè´å°”ç»æµå­¦å¥–çš„å­¦è€… Kahneman åŠå…¶æ­æ¡£ Tverskyã€‚å‰æ™¯ç†è®ºç ”ç©¶äº†äººä»¬å¦‚ä½•å¯¹æœªæ¥äº‹ä»¶åšå‡ºé¢„æµ‹ã€å†³ç­–å’Œè¡Œä¸º
# é€‰æ‹©ï¼Œä»¥åŠè¿™äº›å†³ç­–å’Œè¡Œä¸ºé€‰æ‹©å¦‚ä½•å—åˆ°æƒ…ç»ªã€åè§å’Œå…¶ä»–å¿ƒç†å› ç´ çš„å½±å“ã€‚æŠ•èµ„è€…åœ¨å¤šé¡¹èµ„äº§ä¸­è¿›è¡Œé€‰æ‹©æ—¶ï¼Œå¯
# èƒ½ä¼šå‡ºç°ä¸åŒçš„ç»“æœï¼Œæ¯ä¸ªç»“æœéƒ½å­˜åœ¨ç›¸åº”çš„å‘ç”Ÿæ¦‚ç‡ã€‚å› æ­¤ï¼ŒæŸä¸ªå†³ç­–çš„æœ€ç»ˆä»·å€¼ç­‰äºæ‰€æœ‰å¯èƒ½å‘ç”Ÿçš„ç»“æœçš„æ¦‚
# ç‡åŠ æƒå¹³å‡ï¼ŒæŠ•èµ„è€…ä¼šåœ¨æ‰€æœ‰çš„å†³ç­–ä¸­é€‰æ‹©ä»·å€¼æœ€é«˜çš„ä½œä¸ºæœ€ç»ˆå†³ç­–ï¼Œå³âˆ‘ ğœ‹(ğ‘¥)ğ‘£(ğ‘¥)ï¼Œğœ‹(ğ‘¥)ä¸ºç»“æœå‘ç”Ÿçš„æ¦‚ç‡ï¼Œğ‘£(ğ‘¥)ä¸ºæŸé¡¹é€‰æ‹©æ‰€å…·æœ‰çš„ä»·å€¼ã€‚åŸºäºç´¯ç§¯å‰æ™¯ç†è®ºï¼ŒBarberis ç­‰ï¼ˆ2016ï¼‰åœ¨ä¸ªè‚¡å±‚é¢ä¸Šæ„å»ºäº†æ¯åªè‚¡ç¥¨çš„ TKï¼ˆTverskyã€Kahnemanï¼‰ä»·å€¼ï¼Œè®¤ä¸º TK ä»·å€¼è¾ƒé«˜çš„è‚¡ç¥¨å¯¹äºæŠ•èµ„è€…è€Œè¨€å…·æœ‰æ›´é«˜çš„å¸å¼•åŠ›ï¼Œå› æ­¤æŠ•èµ„è€…å€¾å‘äºè¿‡
# åº¦é«˜ä¼°é«˜ TK çš„è‚¡ç¥¨ï¼Œè€Œä½ä¼°äº†ä½ TK è‚¡ç¥¨çš„ä»·å€¼ï¼Œä¸€ä¸ªåšå¤šä½ TK ä»·å€¼è‚¡ç¥¨ã€åšç©ºé«˜ TK ä»·å€¼è‚¡ç¥¨çš„ç­–ç•¥ç»„åˆèƒ½å¤Ÿåœ¨ç»Ÿè®¡ä¸Šè·å¾—æ˜¾è‘—çš„è¶…é¢æ”¶ç›Šã€‚
# 
# è¿‘å‡ å¹´ï¼Œå‡¸æ˜¾ç†è®ºï¼ˆSalience Theoryï¼‰äº¦æ˜¯è¡Œä¸ºé‡‘èå­¦é¢†åŸŸä¸­å¯¹èµ„äº§å®šä»·å…·æœ‰é‡è¦å½±å“çš„ä¸€ä¸ªçƒ­é—¨ç ”ç©¶æ–¹å‘ã€‚
# BGSï¼ˆ2012ï¼‰è®¤ä¸ºåœ¨èµ„äº§çš„æ¨ªå‘æ¯”è¾ƒä¸­ï¼ŒæŠ•èµ„è€…çš„æ³¨æ„åŠ›å¾€å¾€ä¼šè¢«å¸å¼•åˆ°å¹³å‡è€Œè¨€æœ€å…·æœ‰å‡¸æ˜¾æ€§çš„å›æŠ¥ä¸Šï¼Œè€Œä¸å‡¸æ˜¾çš„å›æŠ¥å¾€å¾€ä¼šè¢«å¿½ç•¥ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯èƒ½åªä¼šè®°å¾—æŸåªè‚¡ç¥¨å½“æœˆå‘ç”Ÿè¿‡æ¶¨åœï¼Œè€Œä¸è®°å¾—å®ƒå¾®æ¶¨ 2%çš„æ—¶å€™ã€‚å› æ­¤ï¼ŒæŠ•èµ„è€…å¯¹äºä¸åŒçš„æ”¶ç›Šå¤§å°ä¼šå­˜åœ¨ä¸åŒçš„å¿ƒç†æƒé‡ï¼Œå°†è¿™ç§å¿ƒç†åå¥½ä»¥å®šé‡çš„å½¢å¼è¿›è¡Œè¡¨è¾¾ï¼Œèƒ½å¤Ÿå¸®åŠ©æ›´åŠ ç²¾ç»†åŒ–åœ°æç»˜èµ„äº§ä»·æ ¼ç›¸å¯¹äºå…¶çœŸå®ä»·å€¼çš„åç¦»ç¨‹åº¦ã€‚Cosemans ç­‰ï¼ˆ2021ï¼‰åŸºäºå‡¸æ˜¾ç†è®ºæ„å»ºäº† ST æŒ‡æ ‡ï¼Œå°†æŠ•èµ„è€…çš„æŠ•èµ„å†³ç­–å¿ƒç†è¿›è¡Œäº†è¿˜åŸã€‚å½“ ST ä¸ºæ­£æ—¶ï¼Œè‚¡ç¥¨çš„æœ€é«˜å›æŠ¥è¾ƒä¸ºçªå‡ºï¼Œå¯¼è‡´æŠ•èµ„è€…è¿‡åº¦å…³æ³¨è‚¡ç¥¨çš„ä¸Šæ¶¨æ½œåŠ›ï¼Œä»è€Œæˆä¸ºé£é™©å¯»æ±‚è€…ï¼›å½“æŠ•èµ„è€…è¿‡åˆ†å…³æ³¨è‚¡ç¥¨çš„è´Ÿæ”¶ç›Šå¹¶å¼ºè°ƒå…¶ä¸‹è¡Œé£é™©æ—¶ï¼ŒST ä¸ºè´Ÿï¼Œç›¸å…³çš„è‚¡ç¥¨å°†é¢ä¸´è¿‡åº¦ä½ä¼°ã€‚
# 
# å°†å‰æ™¯ç†è®ºä¸å‡¸æ˜¾ç†è®ºè¿›è¡Œå¯¹æ¯”å¯ä»¥å‘ç°ï¼šåœ¨å‰æ™¯ç†è®ºä¸­ï¼ŒæŠ•èµ„è€…è¿›è¡ŒæŠ•èµ„å†³ç­–çš„å¿ƒç†æƒé‡åå·®åœ¨äºç»™äºˆäº†å’Œå°¾éƒ¨æ”¶ç›Šç›¸å…³çš„å°æ¦‚ç‡äº‹ä»¶æ›´é«˜çš„æƒé‡ï¼›è€Œåœ¨å‡¸æ˜¾ç†è®ºä¸­ï¼Œæç«¯æ”¶ç›Šè¢«åŠ æƒçš„åŸå› å¹¶ä¸æ˜¯å› ä¸ºå®ƒä»¬çš„å‘ç”Ÿæ¦‚ç‡å°ï¼Œè€Œæ˜¯å› ä¸ºå®ƒä»¬åœ¨æˆªé¢ä¸Šç›¸å¯¹å¸‚åœºå¹³å‡æ”¶ç›Šæ¥è¯´å…·æœ‰å‡¸æ˜¾æ€§ï¼Œå‡¸æ˜¾ç†è®ºæ¨¡å‹è®¤ä¸ºèµ„äº§çš„æº¢ä»·ä¸æ˜¯ç”±æŠ•èµ„è€…çš„åå¥½é©±åŠ¨çš„ï¼Œè€Œæ˜¯ç”±èµ„äº§æ”¶ç›Šç›¸å¯¹å¸‚åœºå¹³å‡æ”¶ç›Šè„±é¢–è€Œå‡ºçš„ç¨‹åº¦é©±åŠ¨çš„ï¼Œå½“ä¸­æ—¢åŒ…å«äº†æ—¶åºä¿¡æ¯ï¼Œä¹ŸåŒ…å«äº†æˆªé¢ä¿¡æ¯ã€‚
# 
# **Step1:**
# 
# æˆ‘ä»¬â¾¸å…ˆè®¡ç®—ç¬¬då¤©è‚¡ç¥¨æ”¶ç›Šå’Œå¸‚åœºæ”¶ç›Šä¹‹é—´çš„è·ç¦»ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
# 
# $$\sigma(r_{i,d})=\frac{|r_{i,d}-\overline{r_{d}}|}{|r_{i,d}|+|\overline{r_{d}}|+\theta} \tag{1}$$
# 
# å…¶ä¸­$r_{i,d}$æ˜¯è‚¡ç¥¨çš„dæ—¥çš„æ—¥åº¦æ”¶ç›Š,$\overline{r_d}$æ˜¯dæ—¥æˆªé¢ä¸Šæ‰€æœ‰è‚¡ç¥¨çš„å¹³å‡æ”¶ç›Šã€‚ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0çš„æƒ…å†µ,$\theta$è®¾ç½®ä¸º0.1ã€‚
# 
# **Step2:**
# 
# ç„¶åå°†æ¯ä¸ªè‚¡ç¥¨æ ¹æ®è¿‡å»ä¸€ä¸ªæœˆæ¯å¤©çš„$\delta(r_{i,d})$è¿›è¡Œæ’åº,è¯¥è‚¡ç¥¨æ¯æ—¥çš„æ’åºä¸º$k_{i,d}$ã€‚æ ¹æ®æ’åºå€¼è®¡ç®—Salience Weights $\omega$:
# 
# $$\omega_{i,d}=\frac{\delta^{k_{i,d}}}{\sum_{d'}{\delta^{k_{i,d}}\pi_{d'}}} \tag{2}$$
# 
# å…¶ä¸­$\delta$(é»˜è®¤ä¸º0.7)æ˜¯ä¸€ä¸ªå‚æ•°,ç”¨äºæ§åˆ¶Salienceæ‰­æ›²çš„ç¨‹åº¦;$\pi=1/N$ã€‚å½“k=1æ—¶,è‚¡ç¥¨æ”¶ç›Šç‡çš„å‡¸æ˜¾æ€§æœ€å¼º,è€Œå½“k=maxæ—¶,è‚¡ç¥¨æ”¶ç›Šç‡çš„å‡¸æ˜¾åº¦æœ€å¼±ã€‚
# 
# **Step3:**
# 
# è®¡ç®—æ¯ä¸ªæœˆæ—¥åº¦çš„$\omega_{i,d}$ä¸æ”¶ç›Šç‡çš„åæ–¹å·®å°±æ˜¯è¯¥è‚¡ç¥¨å½“æœˆçš„STRå€¼
# 
# $$ST=cov(\omega_{i,d},r_{i,d})\tag{3}$$

# In[4]:


# è®¡ç®—w
w: pd.DataFrame = pct_chg.pipe(calc_sigma).pipe(calc_weight)
# è®¡ç®—stå› å­
STR: pd.DataFrame = w.rolling(20).cov(pct_chg)

STR: pd.Series = STR.stack()
STR.name = "STR"


# In[5]:


feature_df: pd.DataFrame = pd.concat((next_ret, STR), axis=1)
feature_df.columns = pd.MultiIndex.from_tuples(
    [("label", "next_ret"), ("feature", "STR")]
)

feature_df.head()


# ## å› å­åˆ†æ

# In[6]:


score_df:pd.DataFrame = feature_df.dropna().copy()
score_df.columns = ['label','score']

model_performance_graph(score_df)


# # æ–¹æ­£è¯åˆ¸æ„é€ 
# 
# æ˜¾è‘—ç†è®ºï¼ˆsalience theoryï¼‰çš„å‡ºç°è§£å†³äº†æˆªâ¾¯ä¸Šåšâ½è¾ƒçš„é—®é¢˜ã€‚Cosemanså’ŒFrehen(2021)ä½¿â½¤æ˜¾è‘—ç† è®ºæ„é€ äº†å› â¼¦ï¼Œä»–ä»¬è®¤ä¸ºé‚£äº›æ”¶ç›Šç‡è¿‡åˆ†â¾¼äºå¸‚åœºæ”¶ç›Šçš„è‚¡ç¥¨ï¼Œä¼šå¸å¼•æŠ•èµ„è€…çš„æ³¨æ„â¼’ï¼Œå¹¶å¼•èµ·æŠ•èµ„è€…çš„ è¿‡åº¦ä¹°â¼Šï¼Œè¿›â½½è‚¡ä»·åœ¨æœªæ¥ä¼šå‘â½£å›è½ã€‚æˆ‘ä»¬å°†è¿™ç§â¼¼ç†ç§°ä¸ºâ€œå®ˆæ ªå¾…å…”â€â¼¼ç†ï¼ŒæŠ•èµ„è€…è®¤ä¸ºè¿™ç§æç«¯åç¦» å¸‚åœºçš„â¾¼æ”¶ç›Šä¼šå†æ¬¡å‡ºç°,å› æ­¤çº·çº·ä¹°â¼Šè¿™äº›è‚¡ç¥¨å¼€å§‹ç­‰å¾…ã€‚
# 
# ç›¸å,é‚£äº›æ”¶ç›Šç‡è¿‡åˆ†ä½äºå¸‚åœºæ”¶ç›Šç‡çš„è‚¡ç¥¨ï¼Œä¼šå¯¹æŠ•èµ„è€…äº§â½£ææ…Œâ¼¼ç†ï¼Œå¹¶å¼•èµ·æŠ•èµ„è€…çš„è¿‡åº¦å–å‡ºï¼Œè¿› â½½è‚¡ä»·åœ¨æœªæ¥å‘â½£è¡¥æ¶¨ã€‚æˆ‘ä»¬å°†è¿™ç§â¼¼ç†ç§°ä¸ºâ€œè‰â½Šçš†å…µâ€â¼¼ç†ï¼ŒæŠ•èµ„è€…è®¤ä¸ºè¿™ç§æç«¯åç¦»å¸‚åœºçš„ä½æ”¶ç›Š ï¼ˆæˆ–ç§°ä¸ºä¸¥é‡äºæŸï¼‰ä¼šå†æ¬¡å‡ºç°,å› æ­¤çº·çº·å–å‡ºè¿™äº›è‚¡ç¥¨,â¼©â¼¼è¿œç¦»å®ƒä»¬ã€‚
# 
# ## åˆå§‹æ„é€ 
# æŠ¥å‘Šæå‡ºäº†**å°†æ˜¾è‘—ç†è®ºä¸åè½¬å› â¼¦ç›¸ç»“åˆçš„æ–°æ„é€ â½…æ³•**ã€‚å³å°†è‚¡ç¥¨æ¯â½‡çš„æ”¶ç›Šç‡,è§†ä½œæŠ•èµ„è€…åšå‡ºå†³ç­–æƒé‡çš„ä¾æ®,å°†æ¯å¤©æ”¶ç›Šç‡åç¦»å¸‚åœºçš„ç¨‹åº¦(æˆ‘ä»¬å–å¤šå¤´ç«¯çš„é€»è¾‘ï¼Œå°†å…¶ç®€ç§°ä¸º"æƒŠæåº¦")ä½œä¸ºæç«¯æ”¶ç›Šå¯¹æŠ•èµ„è€…å†³ç­–æƒé‡çš„æ‰­æ›²ç¨‹åº¦,**ä½¿â½¤"æƒŠæåº¦"ç›´æ¥åŠ æƒæ¯â½‡æ”¶ç›Šç‡**,æ¥æ¨¡æ‹ŸæŠ•èµ„è€…å†³ç­–è¿‡ç¨‹,æ„é€ äº†"åŸå§‹æƒŠæ"å› â¼¦ã€‚
# 
# **å®šä¹‰æƒŠæåº¦**
# Cosemanså’ŒFrehen(2021)ç»™å‡ºäº†è¡¡é‡"æƒŠæåº¦"çš„è®¡ç®—æ–¹æ³•,æœ¬æ–‡å€Ÿé‰´äº†è¯¥æ„é€ æ–¹å¼,å…·ä½“å¦‚ä¸‹:
# 
# 1. å–ä¸­è¯å…¨æŒ‡(000985.SH)æŒ‡æ•°æ”¶ç›Šä½œä¸ºå¸‚åœºæ°´å¹³çš„ä»£è¡¨ï¼Œå°†ä¸­è¯å…¨æŒ‡çš„æ¯æ—¥æ”¶ç›Šç‡(ä»Šæ—¥æ”¶ç›˜æŒ‡æ•°/æ˜¨æ—¥æ”¶ç›˜æŒ‡æ•°-1)ä½œä¸ºä»Šæ—¥å¸‚åœºæ”¶ç›Šç‡æ°´å¹³ã€‚ 
# 2. è®¡ç®—ä¸ªè‚¡æ”¶ç›Šç‡ä¸å¸‚åœºæ”¶ç›Šç‡çš„å·®å€¼,å†å–ç»å¯¹å€¼,ä½œä¸ºä¸ªè‚¡ç›¸å¯¹å¸‚åœºæ”¶ç›Šç‡çš„åç¦»æ°´å¹³,è®°ä¸º"åç¦»é¡¹";è®¡ç®—ä¸ªè‚¡æ”¶ç›Šç‡çš„ç»å¯¹å€¼,åŠ å¸‚åœºæ”¶ç›Šç‡çš„ç»å¯¹å€¼ï¼Œå†åŠ 0.1,ä½œä¸ºå¸‚åœºæ€»ä½“çš„æ”¶ç›Šæ°´å¹³,è®°ä¸º"åŸºå‡†é¡¹"ã€‚ 
# 3. ä½¿ç”¨"åç¦»é¡¹"é™¤ä»¥"åŸºå‡†é¡¹"ï¼Œå¾—åˆ°è¯¥è‚¡ç¥¨åœ¨è¯¥æ—¥çš„"æƒŠæåº¦"ã€‚
# 
# ![avatar](img/20230411_1.png)
# 
# **æƒŠæåº¦å› å­**
# 
# æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨è‚¡ç¥¨æ—¥åº¦äº¤æ˜“æ•°æ®æ„é€ â€œåŸå§‹æƒŠæâ€å› å­ï¼Œå…·ä½“æ­¥éª¤ å¦‚ä¸‹:
# 1. å°†æ¯æ—¥è‚¡ç¥¨æ”¶ç›Šç‡(ä»Šæ”¶/æ˜¨æ”¶-1)ç›´æ¥ä½œä¸ºå½“æ—¥è‚¡ç¥¨çš„å†³ç­–åˆ†ã€‚ 
# 2. å°†æ¯æ—¥çš„"æƒŠæåº¦"ä¸æ¯æ—¥çš„æ”¶ç›Šç‡ç›¸ä¹˜,å¾—åˆ°åŠ æƒè°ƒæ•´åçš„å†³ç­–åˆ†,ç®€ç§°"åŠ æƒå†³ç­–åˆ†"ã€‚ 
# 3. æ¯æœˆæœˆåº•ï¼Œåˆ†åˆ«è®¡ç®—è¿‡å» 20 ä¸ªäº¤æ˜“æ—¥çš„"åŠ æƒå†³ç­–åˆ†"çš„**å‡å€¼**å’Œ**æ ‡å‡†å·®**,åˆ†åˆ«ä½œä¸ºå¯¹"20æ—¥æ”¶ç›Šç‡å› å­"å’Œ"20æ—¥æ³¢åŠ¨ç‡å› å­"çš„æ”¹è¿›ï¼Œåˆ†åˆ«è®°ä¸º"æƒŠææ”¶ç›Š"å› å­å’Œ"æƒŠææ³¢åŠ¨"å› å­,å¹¶å°†äºŒè€…ç­‰æƒåˆæˆä¸º"åŸå§‹æƒŠæ"å› å­ã€‚
# 
# **ä»¥ä¸‹å› å­å˜å½¢æ— éæ ¹æ®é€»è¾‘æ„é€ æƒé‡é¡¹**
# 
# ---
# ## æ³¢åŠ¨ç‡åŠ å‰§
# 
# è®¡ç®—æ¯æ—¥ä¸ªè‚¡çš„æ³¢ åŠ¨ç‡ï¼Œå¹¶å°†å…¶åŠ å…¥æƒé‡çš„éƒ¨åˆ†ï¼Œæ„é€ â€œæ³¢åŠ¨ç‡åŠ å‰§-æƒŠæâ€å› å­ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š
# 1. å–è‚¡ç¥¨ 1 åˆ†é’Ÿé¢‘ç‡çš„è¡Œæƒ…æ•°æ®ï¼Œè®¡ç®—æ¯åˆ†é’Ÿæ”¶ç›˜ä»·ç›¸å¯¹ä¸Šä¸€åˆ†é’Ÿ æ”¶ç›˜ä»·çš„æ¶¨è·Œå¹…ï¼Œå°†å…¨å¤©æ¯åˆ†é’Ÿæ”¶ç›Šç‡æ±‚æ ‡å‡†å·®ï¼Œå¾—åˆ°è¿™ä¸€å¤©è¯¥ä¸ªè‚¡çš„æ³¢åŠ¨ç‡ã€‚
# 2. è®¡ç®—æ¯å¤©æ¯åªè‚¡ç¥¨çš„æ”¶ç›Šç‡å’Œâ€œæƒŠæåº¦â€ã€‚
# 3. å°†æ¯å¤©çš„æ³¢åŠ¨ç‡ã€â€œæƒŠæåº¦â€å’Œæ”¶ç›Šç‡ç›¸ä¹˜ï¼Œä½œä¸ºå½“æ—¥çš„åŠ æƒå†³ç­–åˆ†ã€‚
# 4. æ¯æœˆæœˆåº•ï¼Œåˆ†åˆ«è®¡ç®—è¿‡å» 20 æ—¥çš„åŠ æƒå†³ç­–åˆ†çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œè®° ä¸ºâ€œæ³¢åŠ¨ç‡åŠ å‰§-æƒŠææ”¶ç›Šâ€å› å­å’Œâ€œæ³¢åŠ¨ç‡åŠ å‰§-æƒŠææ³¢åŠ¨â€å› å­ï¼Œå¹¶å°†äºŒè€…ç­‰æƒåˆæˆä¸ºâ€œæ³¢åŠ¨ç‡åŠ å‰§-æƒŠæâ€å› å­ã€‚
# 
# ## ä¸ªäººæŠ•èµ„è€…äº¤æ˜“å æ¯”
# 
# è®¡ç®— æ¯æ—¥ä¸ªè‚¡çš„ä¸ªäººæŠ•èµ„è€…äº¤æ˜“å æ¯”ï¼Œå¹¶å°†å…¶åŠ å…¥æƒé‡çš„éƒ¨åˆ†ï¼Œæ„é€ â€œä¸ªäººæŠ•èµ„è€…äº¤æ˜“æ¯”-æƒŠæâ€å› å­ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š
# 1. å‚è€ƒwind èµ„é‡‘æµæŒ‡æ ‡å®šä¹‰ï¼Œæˆ‘ä»¬å°†å•ç¬”æˆäº¤é‡‘é¢å°äº4ä¸‡å…ƒçš„äº¤æ˜“ï¼Œè§†ä¸ºä¸ªäººæŠ•èµ„è€…äº¤æ˜“ã€‚æˆ‘ä»¬è®¡ç®—æ¯å¤©ä¸ªè‚¡ä¸ªäººæŠ•èµ„è€…å–å‡ºå’Œä¹°å…¥çš„é‡‘é¢å‡å€¼ï¼Œå†é™¤ä»¥ä¸ªè‚¡çš„å½“æ—¥æ€»ä½“æˆäº¤é‡‘é¢ï¼Œå¾—åˆ°å½“æ—¥ä¸ªè‚¡çš„ä¸ªäººæŠ•èµ„è€…äº¤æ˜“æ¯”ã€‚
# 2. å¦‚ä¸Šè¿°è®¡ç®—æ¯å¤©çš„æ”¶ç›Šç‡å’Œâ€œæƒŠæåº¦â€ã€‚
# 3. å°†æ¯å¤©çš„ä¸ªäººæŠ•èµ„è€…äº¤æ˜“æ¯”ã€â€œæƒŠæåº¦â€å’Œæ”¶ç›Šç‡ç›¸ä¹˜ï¼Œä½œä¸ºå½“ æ—¥çš„åŠ æƒå†³ç­–åˆ†ã€‚
# 4. æ¯æœˆæœˆåº•ï¼Œåˆ†åˆ«è®¡ç®—è¿‡å» 20 æ—¥çš„åŠ æƒå†³ç­–åˆ†çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œè®° ä¸ºâ€œä¸ªäººæŠ•èµ„è€…äº¤æ˜“æ¯”-æƒŠææ”¶ç›Šâ€å› å­å’Œâ€œä¸ªäººæŠ•èµ„è€…äº¤æ˜“æ¯”-æƒŠææ³¢åŠ¨â€å› å­ï¼Œå¹¶å°†äºŒè€…ç­‰æƒåˆæˆä¸ºâ€œä¸ªäººæŠ•èµ„è€…äº¤æ˜“æ¯”-æƒŠæâ€å› å­ã€‚
# 
# ## æ³¨æ„åŠ›è¡°å‡
# è€ƒè™‘å°†â€œæƒŠæåº¦â€å‡å»è¿‡å»ä¸¤å¤©çš„å‡å€¼ï¼Œæ„é€ è¡° å‡åçš„â€œæƒŠæåº¦â€ï¼Œå¹¶å°†å…¶åŠ å…¥æƒé‡çš„éƒ¨åˆ†ï¼Œæ„é€ â€œæ³¨æ„åŠ›è¡°å‡-æƒŠæâ€å› å­ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š
# 1. è®¡ç®—æ¯å¤©çš„â€œæƒŠæåº¦â€ï¼Œå°† t æ—¥çš„æƒŠæåº¦ï¼Œå‡å» t-1 æ—¥å’Œ t-2 æ—¥çš„ â€œæƒŠæåº¦â€çš„å‡å€¼ï¼Œå¾—åˆ°ä¸€ä¸ªå·®å€¼ï¼Œç”±äºè¯¥å·®å€¼éœ€è¦ä½œä¸ºæƒé‡ä¿¡æ¯æ¥ ä½¿ç”¨ï¼Œå› æ­¤è¦ä¿è¯æŒ‡æ ‡ä¸ºæ­£æ•°ï¼Œè¿™é‡Œå°†è¯¥å·®å€¼ä¸ºè´Ÿçš„äº¤æ˜“æ—¥çš„æ•°æ®éƒ½æ›¿æ¢ä¸ºç©ºå€¼ï¼Œä»…ä¿ç•™å°† t æ—¥çš„æƒŠæåº¦å¤§äº t-1 æ—¥å’Œ t-2 æ—¥çš„â€œæƒŠæåº¦â€å‡å€¼çš„äº¤æ˜“æ—¥ï¼Œå°†å…¶è®°ä¸ºè¡°å‡åçš„â€œæƒŠæåº¦â€ã€‚
# 2. è®¡ç®—æ¯å¤©çš„æ”¶ç›Šç‡ã€‚
# 3. å°†æ¯å¤©çš„è¡°å‡åçš„â€œæƒŠæåº¦â€å’Œæ”¶ç›Šç‡ç›¸ä¹˜ï¼Œä½œä¸ºå½“æ—¥çš„åŠ æƒå†³ ç­–åˆ†ã€‚
# 4. æ¯æœˆæœˆåº•ï¼Œåˆ†åˆ«è®¡ç®—è¿‡å» 20 æ—¥çš„åŠ æƒå†³ç­–åˆ†çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆç”± äºä¸Šè¿°å·®å€¼ä¸ºè´Ÿçš„æ—¥å­éƒ½æ›¿æ¢ä¸ºäº†ç©ºå€¼ï¼Œå¯¼è‡´è¡°å‡åçš„â€œæƒŠæåº¦â€è¦† ç›–åº¦è¾ƒä½ï¼Œå› æ­¤æœ¬å¤„ä¸ºäº†æé«˜æœ€ç»ˆå› å­è¦†ç›–ç‡ï¼Œåªè¦æ¯æœˆåŠ æƒå†³ç­–åˆ† æ•°æ®è¶³å¤Ÿ 5 æ¡ï¼Œå°±å¯ä»¥è®¡ç®—ï¼Œä¸‹åŒï¼‰ï¼Œè®°ä¸ºâ€œæ³¨æ„åŠ›è¡°å‡-æƒŠææ”¶ç›Šâ€ å› å­å’Œâ€œæ³¨æ„åŠ›è¡°å‡-æƒŠææ³¢åŠ¨â€å› å­ï¼Œå¹¶å°†äºŒè€…ç­‰æƒåˆæˆä¸ºâ€œæ³¨æ„åŠ›è¡°å‡-æƒŠæâ€å› å­ã€‚

# In[7]:


# è®¡ç®—è·å¾—æƒŠæåº¦,å‡†å‡†æ”¶ç›Šä½¿ç”¨çš„æ²ªæ·±300æ”¶ç›Š
sigma: pd.DataFrame = pct_chg.pipe(calc_sigma, bench=bench)
# åŠ æƒå†³ç­–åˆ†
weighted: pd.DataFrame = sigma.mul(pct_chg)
# åŠ æƒå†³ç­–åˆ†å‡å€¼
avg_score: pd.DataFrame = weighted.rolling(20).mean()

avg_score_ser: pd.Series = avg_score.stack()
avg_score_ser.name = "avg_score"

# åŠ æƒå†³ç­–åˆ†æ ‡å‡†å·®
std_score: pd.DataFrame = weighted.rolling(20).std()

std_score_ser: pd.Series = std_score.stack()
std_score_ser.name = "std_score"

# ç­‰æƒåˆæˆæƒŠæåº¦å¾—åˆ† - åç»­å¯ä»¥ç”¨qlibçš„æ¨¡å‹åˆæˆå¯»æ‰¾æœ€ä¼˜
terrified_score: pd.DataFrame = (avg_score + std_score) * 0.5

terrified_score_ser: pd.Series = terrified_score.stack()
terrified_score_ser.name = "terrified_score"

terrified_df: pd.DataFrame = pd.concat(
    (avg_score_ser, std_score_ser, terrified_score_ser, next_ret), axis=1
)
terrified_df.sort_index(inplace=True)

terrified_df.head()


# ## å› å­åˆ†æ

# In[8]:


group_returns: pd.DataFrame = (terrified_df.pipe(pd.DataFrame.dropna)
                                           .pipe(clean_factor_data)
                                           .pipe(get_factor_group_returns, quantile=5))

group_cum:pd.DataFrame = ep.cum_returns(group_returns)


# In[9]:


# ç”»å›¾
for factor_name, df in group_cum.groupby(level=0, axis=1):
    df.plot(title=factor_name, figsize=(12, 6))
    plt.axhline(0, ls="--", color="black")


# ## å› å­å¤åˆ

# In[10]:


test_df:pd.DataFrame = terrified_df[['avg_score','std_score','next_ret']].copy()
test_df.columns = pd.MultiIndex.from_tuples([("feature",'avg_score'),('feature','std_score'),('label',"next_ret")])


# In[11]:


TARIN_PERIODS: Tuple = ("2014-01-01", "2017-12-31")
VALID_PERIODS: Tuple = ("2018-01-01", "2019-12-31")
TEST_PERIODS: Tuple = ("2020-01-01", "2023-02-17")

learn_processors = [DropnaLabel()]
infer_processors = [ProcessInf(), CSRankNorm(), Fillna()]

sdl = StaticDataLoader(config=test_df)
dh_pr = DataHandlerLP(
    instruments=POOLS,
    start_time=TARIN_PERIODS[0],
    end_time=TEST_PERIODS[1],
    process_type=DataHandlerLP.PTYPE_A,
    learn_processors=learn_processors,
    infer_processors=infer_processors,
    data_loader=sdl,
)

ds = DatasetH(
    dh_pr,
    segments={"train": TARIN_PERIODS, "valid": VALID_PERIODS, "test": TEST_PERIODS},
)


# In[12]:


record_dict: Dict = run_model(
    ds,
    "gbdt",
    start_time=TEST_PERIODS[0],
    end_time=TEST_PERIODS[1],
    experiment_name="terrified",
    trained_model="trained_model.pkl",
)


# In[13]:


try:
    recorder = record_dict['recorder']
except NameError:
    # ä½¿ç”¨å·²æœ‰æ¨¡å‹
    from qlib.workflow import R
    import pickle

    with open("../ç­¹ç åˆ†å¸ƒç®—æ³•/factor_data/turnovercoeff_dataset.pkl", "rb") as f:
        turncoeff_dataset = pickle.load(f)

    with R.start():
        recorder = R.get_recorder(
            recorder_name="mlflow_recorder",
            recorder_id="97284ccb8e274ffe83e34fa8f9d84b7e",
        )

label_df = recorder.load_object("label.pkl")
label_df.columns = ["label"]
pred_df: pd.DataFrame = recorder.load_object("pred.pkl")

# åˆ›å»ºæµ‹è¯•é›†"é¢„æµ‹"å’Œâ€œæ ‡ç­¾â€å¯¹ç…§è¡¨
pred_label_df: pd.DataFrame = pd.concat([pred_df, label_df], axis=1, sort=True).reindex(
    label_df.index
)


# In[14]:


model_performance_graph(pred_label_df,duplicates="drop")


# In[15]:


report_normal_1day_df: pd.DataFrame = recorder.load_object(
    "portfolio_analysis/report_normal_1day.pkl")


# In[16]:


report_graph(report_normal_1day_df)


# # å¹¿å‘è¯åˆ¸æ„é€ 
# 
# **ç‰¹ç‚¹:åŠ å…¥é‡çš„ç»´åº¦**
# 
# STRä¸ºä»£è¡¨çš„åœ¨ç¾è‚¡æ„å»ºçš„å‡¸æ˜¾å› å­å¤§å¤šä»â€œä»·â€çš„è§’åº¦åˆ»ç”»äº†æŠ•èµ„è€…æ³¨æ„åŠ›åˆ† é…æœºåˆ¶ã€‚ STRä»â€œä»·â€çš„è§’åº¦åˆ»ç”»å‡¸æ˜¾çš„â€œç›¸å¯¹æ”¶ç›Šç‡â€å¯¹æŠ•èµ„è€…å…³æ³¨çš„å½±å“ä¸æŠ•èµ„å†³ç­–çš„æ‰­æ›²ï¼Œå…¶å…·æœ‰â€œä»·æ ¼èƒ½å……åˆ†åæ˜ æŠ•èµ„è€…å…³æ³¨â€çš„æ½œåœ¨å‡å®šï¼Œè¿™ç¬¦åˆç¾è‚¡å¸‚åœºç‰¹å¾ã€‚ç¾è‚¡ ä¸è®¾ç½®é’ˆå¯¹æ™®éä¸ªè‚¡çš„æ¶¨è·Œåœåˆ¶åº¦ï¼Œä¸”å¯¹æ»¡è¶³æ¡ä»¶çš„è´¦æˆ·é€‚ç”¨T+0äº¤æ˜“è§„åˆ™ï¼Œè¿™ä½¿å¾—ä»·æ ¼èƒ½å……åˆ†å˜åŒ–ï¼Œç¾è‚¡å¸‚åœºçš„â€œä»·â€ä¸å…³æ³¨åº¦é«˜åº¦ç›¸å…³ã€‚
# 
# ç„¶è€Œï¼Œå¦‚æœåœ¨ä¸­å›½å¸‚åœºç…§æ¬STRå› å­çš„æ„å»ºï¼Œå°±å­˜åœ¨è¿èƒŒå‡¸æ˜¾å› å­æ„å»ºçš„æ½œåœ¨ å‡å®šå¯èƒ½ï¼Œä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šå…¶ä¸€ï¼Œæˆ‘å›½Aè‚¡å¸‚åœºå­˜åœ¨æ¶¨è·Œåœé™åˆ¶ï¼Œæç«¯çš„ä»·æ ¼ éƒ½è¢«æˆªå°¾ï¼Œä½¿å¾—Aè‚¡çš„ä»·æ ¼ä¸èƒ½åƒç¾è‚¡ä¸€æ ·å……åˆ†åæ˜ æŠ•èµ„è€…å…³æ³¨ï¼ŒåŒæ—¶ï¼Œæ¶¨è·Œåœåˆ¶åº¦ æœ¬èº«ä¹Ÿå¯¹æŠ•èµ„è€…å…³æ³¨æœ‰é‡è¦å½±å“ï¼ˆç‹æœé˜³ã€ç‹æŒ¯éœï¼Œ2017ï¼›Wangï¼Œ2017ï¼‰ï¼Œæ¥è¿‘æ¶¨ è·Œåœæ—¶çš„â€œç»å¯¹æ”¶ç›Šç‡â€ä¹Ÿä¼šè¢«æŠ•èµ„è€…æå¤§å…³æ³¨ï¼›å…¶äºŒï¼Œå¥—åˆ©é™åˆ¶ä¸æŠ•èµ„è€…ç»“æ„å…±åŒå†³å®šäº†ä¸­å›½å¸‚åœºçš„äº¤æ˜“é‡ä¸å…³æ³¨åº¦é«˜åº¦ç›¸å…³ã€‚
# 
# å› æ­¤ï¼Œå‚è€ƒä½•å®¶ç’‡ç­‰ï¼ˆ2022ï¼‰ï¼Œä»â€œé‡â€çš„è§’åº¦å¹¶ç»“åˆä¸­å›½æ¶¨è·Œåœåˆ¶åº¦å¯¹æŠ•èµ„è€… å…³æ³¨çš„å½±å“ï¼Œæ„å»º**å‡¸æ˜¾å› å­STV**
# 
# ä¸ç¾å›½å¸‚åœºä¸åŒçš„æ˜¯ï¼Œä¸­å›½è‚¡å¸‚è®¾ç½®äº†æ¶¨è·Œåœåˆ¶åº¦ã€‚æ¶¨è·Œå¹…æœºåˆ¶è®¾å®šçš„é˜ˆå€¼æ— æ„ä¸­ç»™æŠ•èµ„è€…ä¸€ä¸ªæ˜ç¡®çš„â€œæ­¢æŸ/æ­¢ç›ˆâ€ç›®æ ‡ï¼ŒæŠ•èµ„è€…èµ¶åœ¨**è§¦å‘10%é˜ˆå€¼**å‰å–å‡ºæˆ–ä¹°å…¥ï¼Œä½¿å¾—æ¶¨è·Œå¹…æ¥è¿‘10%çš„è‚¡ä¼šæ›´åŠ å‡¸æ˜¾ã€‚å› æ­¤ï¼Œæœ¬æ–‡åœ¨è¡¡é‡è‚¡ç¥¨æ”¶ç›ŠçŠ¶æ€çš„å‡¸æ˜¾æ€§æ—¶ï¼Œè®¾ç½®æ”¶ç›Šé˜ˆå€¼ï¼Œå¯¹ä¸åŒæ”¶ç›Šç‡åŒºåˆ†å¤„ç†ï¼š
# 1. æ”¶ç›Šç‡ç»å¯¹å€¼è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œè®¤ä¸ºæ˜¯æ‰€æœ‰æ”¶ç›ŠçŠ¶æ€ä¸­æœ€å‡¸æ˜¾çš„ï¼Œå¤šæ—¥çš„æ”¶ç›Šç‡å‡è¶…è¿‡äº†é˜ˆå€¼ï¼Œåˆ™å®ƒä»¬çš„å‡¸æ˜¾æ€§æŒ‰ç…§æ”¶ç›Šç‡ ç»å¯¹å€¼çš„å¤§å°é™åºæ’åºï¼›
# 2. æ”¶ç›Šç‡ç»å¯¹å€¼æœªè¶…è¿‡é˜ˆå€¼æ—¶ï¼Œå‡¸æ˜¾æ€§åˆ™æŒ‰ç…§å½“æ—¥æ¢æ‰‹ç‡é™åºæ’åºã€‚
# 
# STVçš„å‡¸æ˜¾æ€§å‡½æ•°å¦‚ä¸‹ï¼š
# 
# $$\sigma(turnover_{i,s},r_{i,s})=\begin{cases}|r_{i,s}|*1000,\ |r_{i,s}|\geq X \\
# turnover_{i,s}, \ |r_{i,s}|\lt X
# \end{cases}$$
# 
# $|r_{i,s}|*1000$çš„è®¾å®šæ˜¯ä¸ºäº†ç¡®ä¿ç»å¯¹æ”¶ç›Šç‡è¶…è¿‡Xçš„çŠ¶æ€æ˜¯æœ€å‡¸æ˜¾çš„ã€‚

# In[17]:


def get_stv_feature() -> str:
    abs_ret: str = "Abs($close/Ref($close,1)-1)"
    return f"If({abs_ret}>=0.1,{abs_ret}*100,$turnover_rate)"


# In[18]:


sigma_frame:pd.DataFrame = D.features(POOLS,fields=[get_stv_feature()])

sigma_frame.columns = ['sigma']

sigma_frame:pd.DataFrame = sigma_frame.unstack(level=0)['sigma']


# In[19]:


stv_w:pd.DataFrame = calc_weight(sigma_frame)
STV:pd.DataFrame = stv_w.rolling(20).cov(pct_chg)

STV:pd.Series = STV.stack()
STV.name = "STV"


# In[20]:


feature_stv: pd.DataFrame = pd.concat(
    (next_ret, STV), axis=1
)
feature_stv.columns = pd.MultiIndex.from_tuples(
    [("label", "next_ret"), ("feature", "STV")]
)

feature_stv.head()


# ## å•å› å­åˆ†æ

# In[21]:


stv_score:pd.DataFrame = feature_stv.dropna().copy()
stv_score.columns = ['label','score']

model_performance_graph(stv_score)


# # å¤åˆä»¥ä¸Šå› å­

# In[22]:


all_data: pd.DataFrame = pd.concat(
    [STR, STV, avg_score_ser, std_score_ser, next_ret], axis=1, sort=True
).dropna()

all_data.columns = pd.MultiIndex.from_tuples(
    [
        ("feature", "STR"),
        ("feature", "STV"),
        ("feature", "avg_score"),
        ("feature", "std_score"),
        ("label", "next_ret"),
    ]
)


# In[23]:


TARIN_PERIODS: Tuple = ("2014-01-01", "2017-12-31")
VALID_PERIODS: Tuple = ("2018-01-01", "2019-12-31")
TEST_PERIODS: Tuple = ("2020-01-01", "2023-02-17")

learn_processors = [DropnaLabel()]
infer_processors = [ProcessInf(), CSRankNorm(), Fillna()]

sdl = StaticDataLoader(config=all_data)
dh_pr = DataHandlerLP(
    instruments=POOLS,
    start_time=TARIN_PERIODS[0],
    end_time=TEST_PERIODS[1],
    process_type=DataHandlerLP.PTYPE_A,
    learn_processors=learn_processors,
    infer_processors=infer_processors,
    data_loader=sdl,
)

ds = DatasetH(
    dh_pr,
    segments={"train": TARIN_PERIODS, "valid": VALID_PERIODS, "test": TEST_PERIODS},
)


# In[24]:


record_dict: Dict = run_model(
    ds,
    "gbdt",
    start_time=TEST_PERIODS[0],
    end_time=TEST_PERIODS[1],
    experiment_name="factor",
    trained_model="trained_model.pkl",
)


# In[25]:


try:
    recorder = record_dict['recorder']
except NameError:
    # ä½¿ç”¨å·²æœ‰æ¨¡å‹
    from qlib.workflow import R
    import pickle

    with open("../ç­¹ç åˆ†å¸ƒç®—æ³•/factor_data/turnovercoeff_dataset.pkl", "rb") as f:
        turncoeff_dataset = pickle.load(f)

    with R.start():
        recorder = R.get_recorder(
            recorder_name="mlflow_recorder",
            recorder_id="7eefa18e7b4341f680927657e1de8fa0",
        )

label_df = recorder.load_object("label.pkl")
label_df.columns = ["label"]
pred_df: pd.DataFrame = recorder.load_object("pred.pkl")

# åˆ›å»ºæµ‹è¯•é›†"é¢„æµ‹"å’Œâ€œæ ‡ç­¾â€å¯¹ç…§è¡¨
pred_label_df: pd.DataFrame = pd.concat([pred_df, label_df], axis=1, sort=True).reindex(
    label_df.index
)


# In[26]:


model_performance_graph(pred_label_df,duplicates="drop")


# In[27]:


report_normal_1day_df: pd.DataFrame = recorder.load_object(
    "portfolio_analysis/report_normal_1day.pkl")


# In[28]:


report_graph(report_normal_1day_df)

